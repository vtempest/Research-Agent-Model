

<!DOCTYPE html>
<html lang="en">

<head>
  
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> similarity/topic-distribution.js</title>

  <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>
  <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
  <script src="./build/entry.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css?family=Roboto:100,400,700|Inconsolata,700" rel="stylesheet">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link type="text/css" rel="stylesheet" href="https://jmblog.github.io/color-themes-for-google-code-prettify/themes/tomorrow-night.min.css">
  <link type="text/css" rel="stylesheet" href="styles/app.min.css">
  <link type="text/css" rel="stylesheet" href="styles/iframe.css">
  <link type="text/css" rel="stylesheet" href="">
  <script async defer src="https://buttons.github.io/buttons.js"></script>

  
</head>



<body class="layout small-header">
    <div id="stickyNavbarOverlay"></div>
    

<div class="top-nav">
    <div class="inner">
        <a id="hamburger" role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
        <div class="logo">
            
            
        </div>
        <div class="menu">
            
            <div class="navigation">
                <a
                    href="index.html"
                    class="link"
                >
                    Documentation
                </a>
                
                
                
            </div>
        </div>
    </div>
</div>
    <div id="main">
        <div
            class="sidebar "
            id="sidebarNav"
        >
            
            <nav>
                
                    <h2><a href="index.html">Documentation</a></h2><div class="category"><h3>Global</h3><ul><li><a href="global.html#autocomplete">autocomplete</a></li><li><a href="global.html#calculateCosineSimilarity">calculateCosineSimilarity</a></li><li><a href="global.html#calculatePhraseSpecificity">calculatePhraseSpecificity</a></li><li><a href="global.html#calculateSimilarityByCharacter">calculateSimilarityByCharacter</a></li><li><a href="global.html#calculateSoftmax">calculateSoftmax</a></li><li><a href="global.html#calculateStandardDeviation">calculateStandardDeviation</a></li><li><a href="global.html#convertHTMLToBasicHTML">convertHTMLToBasicHTML</a></li><li><a href="global.html#convertHTMLToTokens">convertHTMLToTokens</a></li><li><a href="global.html#extract">extract</a></li><li><a href="global.html#extractCite">extractCite</a></li><li><a href="global.html#extractPDF">extractPDF</a></li><li><a href="global.html#extractRootDomain">extractRootDomain</a></li><li><a href="global.html#extractSEEKTOPIC">extractSEEKTOPIC</a></li><li><a href="global.html#extractYoutubeText">extractYoutubeText</a></li><li><a href="global.html#fetchViaYoutubeTranscript">fetchViaYoutubeTranscript</a></li><li><a href="global.html#getURLYoutubeVideo">getURLYoutubeVideo</a></li><li><a href="global.html#isNoun">isNoun</a></li><li><a href="global.html#isPhoneNumber">isPhoneNumber</a></li><li><a href="global.html#isUrlPDF">isUrlPDF</a></li><li><a href="global.html#isValidUrl">isValidUrl</a></li><li><a href="global.html#matchQUASAR">matchQUASAR</a></li><li><a href="global.html#parseName">parseName</a></li><li><a href="global.html#recognizeHumanName">recognizeHumanName</a></li><li><a href="global.html#searchSTREAM">searchSTREAM</a></li><li><a href="global.html#searchWeb">searchWeb</a></li><li><a href="global.html#splitArrayToChunks">splitArrayToChunks</a></li><li><a href="global.html#splitSentences">splitSentences</a></li><li><a href="global.html#stemRootWord">stemRootWord</a></li><li><a href="global.html#tokenizeTopics">tokenizeTopics</a></li><li><a href="global.html#vectorizeTextAsConcept">vectorizeTextAsConcept</a></li><li><a href="global.html#weighRelevanceConceptVector">weighRelevanceConceptVector</a></li><li><a href="global.html#weighRelevanceTermFrequency">weighRelevanceTermFrequency</a></li><li><a href="global.html#weighTopicDistributionLDA">weighTopicDistributionLDA</a></li></ul></div>
                
            </nav>
        </div>
        <div class="core" id="main-content-wrapper">
            <div class="content">
                <header class="page-title">
                    <p>Source</p>
                    <h1>similarity/topic-distribution.js</h1>
                </header>
                



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>import stopWords from "../tokenize/stopwords";
/**
 * Latent Dirichlet Allocation creates a docs-topics-words matrix.
 * Latent Dirichlet allocation (pronounced DURSH-ley) is a statistical model used
 * in natural language processing to discover abstract topics in a
 * collection of documents. It is a generative probabilistic model
 * that assumes documents are mixtures of topics, where a topic
 * is a probability distribution over words. LDA uses Bayesian
 * inference to simultaneously learn the topics and topic mixtures
 * for each document in an unsupervised manner.
 * https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation
 * https://www.youtube.com/watch?v=gWgsKyEjclw
 * https://www.youtube.com/watch?v=nfBNOWv1pgE
 * https://www.geeksforgeeks.org/latent-dirichlet-allocation/
 *
 * @param {string[]} sentences - Array of input sentences.
 * @param {Object} options - Configuration options for LDA.
 * @param {number} [options.topicCount=10] - Number of topics to extract.
 * @param {number} [options.numberOfTermsPerTopic=10] - Number of terms to show for each topic.
 * @param {number} [options.alpha=0.1] - Dirichlet prior on document-topic distributions.
 * @param {number} [options.beta=0.01] - Dirichlet prior on topic-word distributions.
 * @param {number} [options.numberOfIterations=1000] - Number of iterations for the LDA algorithm.
 * @param {number} [options.valueBurnIn=100] - Number of burn-in iterations.
 * @param {number} [options.valueSampleLag=10] - Lag between samples.
 * @returns {Array} - Array of topics, each containing term-probability pairs.
 */
export function weighTopicDistributionLDA(sentences, options = {}) {
  const {
    topicCount = 10,
    numberOfTermsPerTopic = 10,
    alpha = 0.1,
    beta = 0.01,
    numberOfIterations = 1000,
    valueBurnIn = 100,
    valueSampleLag = 10
  } = options;

  let processedDocuments = [];
  let termFrequency = {};
  let vocabulary = [];
  let originalVocabulary = {};
  const combinedStopwords = stopWords;

  // Preprocess documents
  sentences.forEach((sentence, sentenceIndex) => {
    if (!sentence) return;

    let words = sentence.split(/[\s,\"]+/);
    if (!words) return;

    processedDocuments[sentenceIndex] = [];

    words.forEach((word) => {
      let cleanedWord = word
        .toLowerCase()
        .replace(/[^a-z\'A-Z0-9\u00C0-\u00ff ]+/g, "");
      let stemmedWord = cleanedWord; //stemmer(cleanedWord);

      if (
        cleanedWord !== "" &amp;&amp;
        stemmedWord &amp;&amp;
        cleanedWord.length > 1 &amp;&amp;
        !combinedStopwords.includes(cleanedWord.replace("'", "")) &amp;&amp;
        !combinedStopwords.includes(stemmedWord)
      ) {
        if (termFrequency[stemmedWord]) {
          termFrequency[stemmedWord]++;
        } else {
          termFrequency[stemmedWord] = 1;
          vocabulary.push(stemmedWord);
          originalVocabulary[stemmedWord] = cleanedWord;
        }

        processedDocuments[sentenceIndex].push(vocabulary.indexOf(stemmedWord));
      }
    });
  });

  // LDA variables
  const vocabularySize = vocabulary.length;
  const documentCount = processedDocuments.length;

  // Initialize LDA state
  let topicAssignments = processedDocuments.map((doc) =>
    doc.map(() => Math.floor(Math.random() * topicCount))
  );

  let wordTopicCounts = Array.from({ length: vocabularySize }, () =>
    new Array(topicCount).fill(0)
  );
  let docTopicCounts = Array.from({ length: documentCount }, () =>
    new Array(topicCount).fill(0)
  );
  let wordTopicTotals = new Array(topicCount).fill(0);
  let docTopicTotals = processedDocuments.map((doc) => doc.length);

  let topicDocumentDist = Array.from({ length: documentCount }, () =>
    new Array(topicCount).fill(0)
  );
  let topicWordDist = Array.from({ length: topicCount }, () =>
    new Array(vocabularySize).fill(0)
  );
  let statsCount = 0;

  processedDocuments.forEach((doc, docIndex) => {
    doc.forEach((wordIndex, position) => {
      const topic = topicAssignments[docIndex][position];
      wordTopicCounts[wordIndex][topic]++;
      docTopicCounts[docIndex][topic]++;
      wordTopicTotals[topic]++;
    });
  });

  // Gibbs sampling
  for (let iteration = 0; iteration &lt; numberOfIterations; iteration++) {
    processedDocuments.forEach((doc, docIndex) => {
      doc.forEach((wordIndex, position) => {
        const currentTopic = topicAssignments[docIndex][position];

        // Decrease counts for current topic assignment
        wordTopicCounts[wordIndex][currentTopic]--;
        docTopicCounts[docIndex][currentTopic]--;
        wordTopicTotals[currentTopic]--;
        docTopicTotals[docIndex]--;

        // Calculate topic probabilities
        const topicProbs = new Array(topicCount).fill(0);
        for (let topic = 0; topic &lt; topicCount; topic++) {
          topicProbs[topic] =
            (((wordTopicCounts[wordIndex][topic] + beta) /
              (wordTopicTotals[topic] + vocabularySize * beta)) *
              (docTopicCounts[docIndex][topic] + alpha)) /
            (docTopicTotals[docIndex] + topicCount * alpha);
        }

        // Sample new topic
        const sum = topicProbs.reduce((a, b) => a + b, 0);
        const normalized = topicProbs.map((p) => p / sum);
        const r = Math.random();
        let cumulativeProb = 0;
        for (let i = 0; i &lt; normalized.length; i++) {
          cumulativeProb += normalized[i];
          if (r &lt; cumulativeProb) return i;
        }
        const newTopic = normalized.length - 1;

        topicAssignments[docIndex][position] = newTopic;

        // Increase counts for new topic assignment
        wordTopicCounts[wordIndex][newTopic]++;
        docTopicCounts[docIndex][newTopic]++;
        wordTopicTotals[newTopic]++;
        docTopicTotals[docIndex]++;
      });
    });

    // Update iteration status and collect samples if necessary
    if (iteration > valueBurnIn &amp;&amp; valueSampleLag > 0 &amp;&amp; iteration % valueSampleLag === 0) {
      for (let docIndex = 0; docIndex &lt; documentCount; docIndex++) {
        for (let topic = 0; topic &lt; topicCount; topic++) {
          topicDocumentDist[docIndex][topic] +=
            (docTopicCounts[docIndex][topic] + alpha) /
            (docTopicTotals[docIndex] + topicCount * alpha);
        }
      }
      for (let topic = 0; topic &lt; topicCount; topic++) {
        for (let wordIndex = 0; wordIndex &lt; vocabularySize; wordIndex++) {
          topicWordDist[topic][wordIndex] +=
            (wordTopicCounts[wordIndex][topic] + beta) /
            (wordTopicTotals[topic] + vocabularySize * beta);
        }
      }
      statsCount++;
    }
  }

  // Calculate final topic-word distribution
  let finalTopicWordDist;
  if (valueSampleLag > 0) {
    finalTopicWordDist = topicWordDist.map((row) =>
      row.map((val) => val / statsCount)
    );
  } else {
    finalTopicWordDist = wordTopicCounts.map((row, wordIndex) =>
      row.map(
        (count, topic) =>
          (count + beta) / (wordTopicTotals[topic] + vocabularySize * beta)
      )
    );
  }

  // Extract and format results
  let topicResults = [];
  finalTopicWordDist.forEach((topic, topicIndex) => {
    let topicTerms = topic.map((prob, wordIndex) => ({
      probability: prob,
      stemmedWord: vocabulary[wordIndex],
      originalWord: originalVocabulary[vocabulary[wordIndex]],
    }));

    topicTerms.sort((a, b) => b.probability - a.probability);

    // Always take the top numberOfTermsPerTopic terms, regardless of probability
    let topicResult = topicTerms
      .slice(0, numberOfTermsPerTopic)
      .map((term) => ({
        term: term.originalWord,
        probability: term.probability,
      }));

    topicResults.push(topicResult);
  });

  return topicResults;
}
</code></pre>
        </article>
    </section>




            </div>
            
            <footer class="footer">
                <div class="content has-text-centered">
                    <p>Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 4.0.3</a></p>
                    <p class="sidebar-created-by">
                        <a href="https://github.com/SoftwareBrothers/better-docs" target="_blank">BetterDocs theme</a> provided with <i class="fas fa-heart"></i> by
                        <a href="http://softwarebrothers.co" target="_blank">SoftwareBrothers - JavaScript Development Agency</a>
                    </p>
                </div>
            </footer>
            
        </div>
        <div id="side-nav" class="side-nav">
        </div>
    </div>
<script src="scripts/app.min.js"></script>
<script>PR.prettyPrint();</script>
<script src="scripts/linenumber.js"> </script>


</body>
</html>
